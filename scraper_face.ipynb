{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e24ce683",
   "metadata": {},
   "source": [
    "# Scraping Facebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f7acbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports et Configuration\n",
    "import time\n",
    "import random\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "\n",
    "# Configuration globale\n",
    "EMAIL = os.getenv('EMAIL') # À changer pour la sécurité\n",
    "PASSWORD = os.getenv('PASSWORD')  # À changer pour la sécurité \n",
    "\n",
    "\n",
    "# Liste des profils à scraper - AJOUTEZ VOS URLS ICI\n",
    "PROFILE_URLS = [\n",
    "    \"https://www.facebook.com/uvburkina\",\n",
    "    # Ajoutez d'autres URLs ici :\n",
    "    # \"https://www.facebook.com/autre_profil\",\n",
    "    # \"https://www.facebook.com/encore_un_profil\",\n",
    "]\n",
    "\n",
    "# Clés pour rechercher les postes par mot-clés\n",
    "KEYWORDS = []\n",
    "\n",
    "DEFAULT_PROFILE_URL = \"https://www.facebook.com/uvburkina\"\n",
    "OUTPUT_FOLDER = \"facebook_data\"\n",
    "SCROLL_STEP = 500\n",
    "SCROLL_DELAY = 2\n",
    "\n",
    "# Créer le dossier de sortie si nécessaire\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "\n",
    "print(\" Configuration chargée\")\n",
    "print(f\" Dossier de sortie: {OUTPUT_FOLDER}\")\n",
    "print(f\" {len(PROFILE_URLS)} profil(s) configuré(s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b9e24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonctions d'initialisation du driver\n",
    "def initialize_driver():\n",
    "    \"\"\"Initialise le driver Chrome avec options anti-détection\"\"\"\n",
    "    try:\n",
    "        options = webdriver.ChromeOptions()\n",
    "        options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "        options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        options.add_argument(\"--no-sandbox\")\n",
    "        options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "        options.add_experimental_option(\"useAutomationExtension\", False)\n",
    "        \n",
    "        driver = webdriver.Chrome(options=options)\n",
    "        driver.execute_script(\"Object.defineProperty(navigator, 'webdriver', {get: () => undefined})\")\n",
    "        \n",
    "        print(\" Driver initialisé avec succès\")\n",
    "        return driver\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" Erreur lors de l'initialisation du driver: {e}\")\n",
    "        return None\n",
    "\n",
    "def close_driver(driver):\n",
    "    \"\"\"Ferme le driver de manière sécurisée\"\"\"\n",
    "    if driver:\n",
    "        try:\n",
    "            driver.quit()\n",
    "            print(\" Driver fermé avec succès\")\n",
    "        except Exception as e:\n",
    "            print(f\" Erreur lors de la fermeture du driver: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6290587f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Fonctions de simulation humaine\n",
    "def simulate_human_typing(element, text):\n",
    "    \"\"\"Simule une frappe humaine réaliste\"\"\"\n",
    "    for char in text:\n",
    "        element.send_keys(char)\n",
    "        time.sleep(random.uniform(0.1, 0.3))\n",
    "        if random.random() < 0.1:  # Pause aléatoire 10% du temps\n",
    "            time.sleep(random.uniform(0.3, 0.7))\n",
    "\n",
    "def slow_scroll(driver, step=SCROLL_STEP):\n",
    "    \"\"\"Fait défiler la page lentement\"\"\"\n",
    "    driver.execute_script(f\"window.scrollBy(0, {step});\")\n",
    "    time.sleep(SCROLL_DELAY)\n",
    "    print(f\" Scroll de {step}px effectué\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7475c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonctions de connexion\n",
    "def login_to_facebook(driver, email=EMAIL, password=PASSWORD):\n",
    "    \"\"\"Se connecte à Facebook\"\"\"\n",
    "    try:\n",
    "        driver.get(\"https://www.facebook.com/login\")\n",
    "        print(\" Accès à la page de connexion Facebook\")\n",
    "        \n",
    "        # Attendre et saisir l'email\n",
    "        email_input = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.NAME, \"email\"))\n",
    "        )\n",
    "        simulate_human_typing(email_input, email)\n",
    "        print(\" Email saisi\")\n",
    "        \n",
    "        # Attendre et saisir le mot de passe\n",
    "        password_input = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.NAME, \"pass\"))\n",
    "        )\n",
    "        simulate_human_typing(password_input, password)\n",
    "        print(\" Mot de passe saisi\")\n",
    "        \n",
    "        # Cliquer sur le bouton de connexion\n",
    "        login_button = driver.find_element(By.XPATH, \"//button[@type='submit']\")\n",
    "        ActionChains(driver)\\\n",
    "            .move_to_element(login_button)\\\n",
    "            .pause(random.uniform(0.2, 0.4))\\\n",
    "            .click()\\\n",
    "            .perform()\n",
    "        \n",
    "        print(\" Tentative de connexion...\")\n",
    "        time.sleep(15)  # Attendre la connexion\n",
    "        print(\" Connexion réussie\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" Erreur lors de la connexion: {e}\")\n",
    "        return False\n",
    "\n",
    "def navigate_to_profile(driver, profile_url):\n",
    "    \"\"\"Navigue vers un profil Facebook spécifique\"\"\"\n",
    "    try:\n",
    "        driver.get(profile_url)\n",
    "        time.sleep(4)\n",
    "        print(f\" Navigation vers le profil: {profile_url}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\" Erreur lors de la navigation: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321fc738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonctions utilitaires pour les profils\n",
    "def extract_profile_name(profile_url):\n",
    "    \"\"\"Extrait le nom du profil depuis l'URL\"\"\"\n",
    "    try:\n",
    "        if '/profile.php?id=' in profile_url:\n",
    "            # Pour les profils avec ID numérique\n",
    "            profile_id = profile_url.split('id=')[1].split('&')[0]\n",
    "            return f\"profile_{profile_id}\"\n",
    "        else:\n",
    "            # Pour les profils avec nom personnalisé\n",
    "            profile_name = profile_url.rstrip('/').split('/')[-1]\n",
    "            return profile_name.replace('.', '_')\n",
    "    except:\n",
    "        return f\"profile_{int(time.time())}\"  # Fallback avec timestamp\n",
    "\n",
    "def add_profile_urls(new_urls):\n",
    "    \"\"\"Ajoute de nouvelles URLs à la liste globale\"\"\"\n",
    "    global PROFILE_URLS\n",
    "    \n",
    "    if isinstance(new_urls, str):\n",
    "        new_urls = [new_urls]\n",
    "    \n",
    "    for url in new_urls:\n",
    "        if url not in PROFILE_URLS:\n",
    "            PROFILE_URLS.append(url)\n",
    "            print(f\" URL ajoutée: {url}\")\n",
    "        else:\n",
    "            print(f\" URL déjà présente: {url}\")\n",
    "    \n",
    "    print(f\" Total URLs dans la liste: {len(PROFILE_URLS)}\")\n",
    "    return PROFILE_URLS\n",
    "\n",
    "def show_current_urls():\n",
    "    \"\"\"Affiche la liste actuelle des URLs\"\"\"\n",
    "    print(f\" URLs configurées ({len(PROFILE_URLS)}):\")\n",
    "    for i, url in enumerate(PROFILE_URLS, 1):\n",
    "        profile_name = extract_profile_name(url)\n",
    "        print(f\"  {i}. {profile_name} - {url}\")\n",
    "    return PROFILE_URLS\n",
    "\n",
    "def remove_url(url_or_index):\n",
    "    \"\"\"Supprime une URL par son adresse ou son index\"\"\"\n",
    "    global PROFILE_URLS\n",
    "    \n",
    "    try:\n",
    "        if isinstance(url_or_index, int):\n",
    "            # Suppression par index\n",
    "            if 0 <= url_or_index < len(PROFILE_URLS):\n",
    "                removed_url = PROFILE_URLS.pop(url_or_index)\n",
    "                print(f\" URL supprimée: {removed_url}\")\n",
    "            else:\n",
    "                print(f\" Index invalide: {url_or_index}\")\n",
    "        else:\n",
    "            # Suppression par URL\n",
    "            if url_or_index in PROFILE_URLS:\n",
    "                PROFILE_URLS.remove(url_or_index)\n",
    "                print(f\" URL supprimée: {url_or_index}\")\n",
    "            else:\n",
    "                print(f\" URL non trouvée: {url_or_index}\")\n",
    "    except Exception as e:\n",
    "        print(f\" Erreur lors de la suppression: {e}\")\n",
    "    \n",
    "    return PROFILE_URLS\n",
    "\n",
    "def clear_all_urls():\n",
    "    \"\"\"Vide la liste des URLs\"\"\"\n",
    "    global PROFILE_URLS\n",
    "    PROFILE_URLS.clear()\n",
    "    print(\" Liste des URLs vidée\")\n",
    "    return PROFILE_URLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66de122b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonctions d'extraction des données\n",
    "def extract_posts_from_page(driver, profile_url=None):\n",
    "    \"\"\"Extrait les données des posts de la page actuelle\"\"\"\n",
    "    try:\n",
    "        page_source = driver.page_source\n",
    "        soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "        posts_data = []\n",
    "        \n",
    "        # Extraire le nom du profil pour l'identifier\n",
    "        profile_name = extract_profile_name(profile_url) if profile_url else \"unknown_profile\"\n",
    "        \n",
    "        posts = soup.find_all(\"div\", {\"class\": \"x1n2onr6 x1ja2u2z\"})\n",
    "        \n",
    "        for post in posts:\n",
    "            try:\n",
    "                # Extraction du texte du post (titre)\n",
    "                message_elements = post.find_all(\"div\", {\"data-ad-preview\": \"message\"})\n",
    "                post_text = \" \".join([msg.get_text(strip=True) for msg in message_elements])\n",
    "                \n",
    "                # Extraction des commentaires\n",
    "                comments = []\n",
    "                comment_elements = post.find_all(\"div\", {\"class\": \"xdj266r x14z9mp xat24cr x1lziwak x1vvkbs\"})\n",
    "                for comment in comment_elements:\n",
    "                    comment_text = comment.get_text(strip=True)\n",
    "                    if comment_text and len(comment_text) > 20:  # Filtrer les textes trop courts\n",
    "                        comments.append(comment_text)\n",
    "                \n",
    "                if post_text:  # Seulement ajouter si il y a du contenu\n",
    "                    posts_data.append({\n",
    "                        \"profile_name\": profile_name,\n",
    "                        \"profile_url\": profile_url,\n",
    "                        \"post_text\": post_text,\n",
    "                        \"comments\": \" | \".join(comments),  # Séparer les commentaires par des |\n",
    "                        \"extracted_at\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                    })\n",
    "            except Exception as e:\n",
    "                print(f\" Erreur lors de l'extraction d'un post: {e}\")\n",
    "                continue\n",
    "                \n",
    "        print(f\" {len(posts_data)} posts extraits de cette page ({profile_name})\")\n",
    "        return posts_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" Erreur lors de l'extraction des posts: {e}\")\n",
    "        return []\n",
    "\n",
    "def remove_duplicates(data_list):\n",
    "    \"\"\"Supprime les posts en double\"\"\"\n",
    "    seen = set()\n",
    "    unique_data = []\n",
    "    for data in data_list:\n",
    "        # Utiliser le texte du post comme identifiant unique\n",
    "        identifier = data.get('post_text', '')\n",
    "        if identifier and identifier not in seen:\n",
    "            seen.add(identifier)\n",
    "            unique_data.append(data)\n",
    "    \n",
    "    print(f\" {len(data_list) - len(unique_data)} doublons supprimés\")\n",
    "    return unique_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c443ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonctions de scraping\n",
    "def scrape_facebook_posts(driver, max_posts=10, profile_url=DEFAULT_PROFILE_URL):\n",
    "    \"\"\"Fonction principale de scraping des posts Facebook pour un profil\"\"\"\n",
    "    profile_name = extract_profile_name(profile_url)\n",
    "    print(f\" Début du scraping de {profile_name} - Objectif: {max_posts} posts\")\n",
    "    \n",
    "    # Navigation vers le profil\n",
    "    if not navigate_to_profile(driver, profile_url):\n",
    "        return []\n",
    "    \n",
    "    all_posts = []\n",
    "    scroll_attempts = 0\n",
    "    max_scroll_attempts = 50  # Limite de sécurité\n",
    "    \n",
    "    while len(all_posts) < max_posts and scroll_attempts < max_scroll_attempts:\n",
    "        # Extraire les posts de la page actuelle\n",
    "        posts = extract_posts_from_page(driver, profile_url)\n",
    "        all_posts.extend(posts)\n",
    "        \n",
    "        # Supprimer les doublons\n",
    "        all_posts = remove_duplicates(all_posts)\n",
    "        \n",
    "        print(f\" Progression {profile_name}: {len(all_posts)}/{max_posts} posts uniques collectés\")\n",
    "        \n",
    "        # Si on a assez de posts, arrêter\n",
    "        if len(all_posts) >= max_posts:\n",
    "            break\n",
    "        \n",
    "        # Faire défiler pour charger plus de contenu\n",
    "        slow_scroll(driver)\n",
    "        scroll_attempts += 1\n",
    "        \n",
    "        # Attendre un peu pour que le nouveau contenu se charge\n",
    "        time.sleep(random.uniform(2, 4))\n",
    "    \n",
    "    # Limiter au nombre demandé\n",
    "    final_posts = all_posts[:max_posts]\n",
    "    print(f\" Scraping de {profile_name} terminé: {len(final_posts)} posts collectés\")\n",
    "    \n",
    "    return final_posts\n",
    "\n",
    "def scrape_multiple_profiles(driver, profile_urls, max_posts_per_profile=10):\n",
    "    \"\"\"Scrape plusieurs profils Facebook\"\"\"\n",
    "    all_profiles_data = []\n",
    "    \n",
    "    print(f\" Scraping de {len(profile_urls)} profils avec {max_posts_per_profile} posts chacun\")\n",
    "    \n",
    "    for i, url in enumerate(profile_urls, 1):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\" Scraping du profil {i}/{len(profile_urls)}: {url}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        try:\n",
    "            posts_data = scrape_facebook_posts(driver, max_posts_per_profile, url)\n",
    "            \n",
    "            if posts_data:\n",
    "                all_profiles_data.extend(posts_data)\n",
    "                print(f\" Profil {i} terminé: {len(posts_data)} posts collectés\")\n",
    "            else:\n",
    "                print(f\" Aucun post collecté pour le profil {i}\")\n",
    "            \n",
    "            # Pause entre les profils pour éviter la détection\n",
    "            if i < len(profile_urls):  # Pas de pause après le dernier profil\n",
    "                wait_time = random.uniform(5, 10)\n",
    "                print(f\" Pause de {wait_time:.1f}s avant le prochain profil...\")\n",
    "                time.sleep(wait_time)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\" Erreur lors du scraping du profil {i} ({url}): {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\n Scraping multi-profils terminé!\")\n",
    "    print(f\" Total: {len(all_profiles_data)} posts collectés sur {len(profile_urls)} profils\")\n",
    "    \n",
    "    return all_profiles_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a020676",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Fonctions de sauvegarde\n",
    "def save_to_csv(posts_data, filename=None):\n",
    "    \"\"\"Sauvegarde les données dans un fichier CSV bien formaté\"\"\"\n",
    "    if not posts_data:\n",
    "        print(\" Aucune donnée à sauvegarder\")\n",
    "        return None\n",
    "    \n",
    "    # Générer un nom de fichier si non fourni\n",
    "    if filename is None:\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d _%H%M%S\")\n",
    "        filename = f\"facebook_posts_comments_{timestamp}.csv\"\n",
    "    \n",
    "    filepath = os.path.join(OUTPUT_FOLDER, filename)\n",
    "    \n",
    "    try:\n",
    "        # Définir les colonnes dans l'ordre souhaité\n",
    "        fieldnames = ['profile_name', 'profile_url', 'post_text', 'comments', 'extracted_at']\n",
    "        \n",
    "        with open(filepath, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "            \n",
    "            # Écrire l'en-tête\n",
    "            writer.writeheader()\n",
    "            \n",
    "            # Écrire les données\n",
    "            for post in posts_data:\n",
    "                writer.writerow(post)\n",
    "        \n",
    "        print(f\" Données sauvegardées dans: {filepath}\")\n",
    "        return filepath\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" Erreur lors de la sauvegarde CSV: {e}\")\n",
    "        return None\n",
    "\n",
    "def display_posts_summary(posts_data):\n",
    "    \"\"\"Affiche un résumé des posts collectés\"\"\"\n",
    "    if not posts_data:\n",
    "        print(\" Aucune donnée à afficher\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\n RÉSUMÉ DES POSTS COLLECTÉS\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Nombre total de posts: {len(posts_data)}\")\n",
    "    \n",
    "    # Statistiques par profil\n",
    "    profiles_stats = {}\n",
    "    for post in posts_data:\n",
    "        profile_name = post.get('profile_name', 'unknown')\n",
    "        if profile_name not in profiles_stats:\n",
    "            profiles_stats[profile_name] = 0\n",
    "        profiles_stats[profile_name] += 1\n",
    "    \n",
    "    print(f\"Nombre de profils scrapés: {len(profiles_stats)}\")\n",
    "    for profile, count in profiles_stats.items():\n",
    "        print(f\"  - {profile}: {count} posts\")\n",
    "    \n",
    "    print(f\"\\n APERÇU DES PREMIERS POSTS:\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    for idx, post in enumerate(posts_data[:3], 1):\n",
    "        print(f\"\\nPost {idx} ({post.get('profile_name', 'unknown')}):\")\n",
    "        print(f\" Profil: {post.get('profile_url', 'N/A')}\")\n",
    "        print(f\" Titre: {post['post_text'][:100]}...\")\n",
    "        print(f\" Commentaires: {post['comments'][:100]}...\")\n",
    "        print(f\"{'-'*30}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f3bf22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction d'exécution complète\n",
    "def run_complete_scraping(max_posts=10, profile_urls=None):\n",
    "    \"\"\"Exécute le scraping complet avec sauvegarde pour un ou plusieurs profils\"\"\"\n",
    "    driver = None\n",
    "    \n",
    "    # Utiliser la liste par défaut si aucune URL fournie\n",
    "    if profile_urls is None:\n",
    "        profile_urls = PROFILE_URLS\n",
    "    \n",
    "    # Convertir en liste si une seule URL est fournie\n",
    "    if isinstance(profile_urls, str):\n",
    "        profile_urls = [profile_urls]\n",
    "    \n",
    "    print(f\" Démarrage du scraping de {len(profile_urls)} profil(s)\")\n",
    "    \n",
    "    try:\n",
    "        # Initialiser le driver\n",
    "        driver = initialize_driver()\n",
    "        if not driver:\n",
    "            return None\n",
    "        \n",
    "        # Se connecter à Facebook\n",
    "        if not login_to_facebook(driver):\n",
    "            return None\n",
    "        \n",
    "        # Scraper les posts\n",
    "        if len(profile_urls) == 1:\n",
    "            # Scraping d'un seul profil\n",
    "            posts_data = scrape_facebook_posts(driver, max_posts, profile_urls[0])\n",
    "        else:\n",
    "            # Scraping de plusieurs profils\n",
    "            posts_data = scrape_multiple_profiles(driver, profile_urls, max_posts)\n",
    "        \n",
    "        if not posts_data:\n",
    "            print(\" Aucun post n'a pu être collecté\")\n",
    "            return None\n",
    "        \n",
    "        # Afficher le résumé\n",
    "        display_posts_summary(posts_data)\n",
    "        \n",
    "        # Sauvegarder les données dans un seul fichier CSV\n",
    "        csv_file = save_to_csv(posts_data)\n",
    "        \n",
    "        print(f\"\\n Scraping terminé avec succès!\")\n",
    "        print(f\" Fichier sauvegardé: {csv_file}\")\n",
    "        \n",
    "        return posts_data, csv_file\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" Erreur générale: {e}\")\n",
    "        return None\n",
    "        \n",
    "    finally:\n",
    "        close_driver(driver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c040731",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# Exemples d'utilisation\n",
    "print(\"\\n CODE PRÊT ! Voici comment utiliser le scraper multi-profils :\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Exécution\n",
    "if __name__ == \"__main__\": \n",
    "    \n",
    "    run_complete_scraping()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
